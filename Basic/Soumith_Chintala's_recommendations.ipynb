{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzZNlKcwSYvbMiaTNp2l9F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Soumith Chintala, one of the co-authors of the DCGAN paper, made a presentation at NIPS 2016 titled How to Train a GAN?*\n",
        "His recommendations are :"
      ],
      "metadata": {
        "id": "hi6xH1NS4o7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWU70iUj3gSA",
        "outputId": "baaac3b4-5d4b-4eb3-c98d-084aac699b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Space shape : (500, 100), Mean : 0.007205884781732588, SD 0.9979680296881249\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "1   Using a Gaussian latent space as input for the generator.\n",
        "    This Gaussian distribution is a normal distribution of mean = 0 and std = 1\n",
        "    such a random distribution is generate using numpy.random.randn()\n",
        "\n",
        "    damn numpy really is something\n",
        "\n",
        "    the shape of (500, 100) : there are 500 samples and each hasd 100 values, all of them are random,\n",
        "    but centered around 0 with the std dev of 1.\n",
        "'''\n",
        "\n",
        "from numpy.random import randn\n",
        "\n",
        "def latent_space(n_dims, n_samples):\n",
        "\n",
        "    x = randn(n_dims * n_samples)\n",
        "    x = x.reshape((n_samples, n_dims))\n",
        "\n",
        "    return x\n",
        "\n",
        "n_dims = 100\n",
        "n_samples = 500\n",
        "\n",
        "samples = latent_space(100, 500)\n",
        "\n",
        "print(f'Space shape : {samples.shape}, Mean : {samples.mean()}, SD {samples.std()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "2   Training the Discriminator on real and fake batches separately rather than combining the two into a single mixed batch.\n",
        "    this requires two calls to the train_on_batch()\n",
        "    one using real inputs\n",
        "    and the other using fake ones (generated ones)\n",
        "'''\n",
        "\n",
        "X_real, y_real = None, None # assuming we assign these from the dataset\n",
        "X_fake, y_fake = None, None # assuming we assign these using generator output with 'real' label\n",
        "\n",
        "# D model training assuming we have defined a discriminator\n",
        "discriminator.train_on_batch(X_real, y_real)\n",
        "discriminator.train_on_batch(X_fake, y_fake)"
      ],
      "metadata": {
        "id": "hj1Lfi5c8R-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "3   Use smooth labels instead of hard/crisp labels\n",
        "    instead of label 1 for real and 0 for fake,\n",
        "    use values close to 1 for real and close to 0 for fake\n",
        "\n",
        "    this has a regularization effect when training the model\n",
        "\n",
        "    the numpy.random.random() returns random floats in the half-open interval [0.0, 1.0)\n",
        "    which is useful for this purpose\n",
        "\n",
        "    numpy.ones() returns a new array of given shape and type, filled with ones,\n",
        "    numpy.zeroes() is the same for zeroes. both are useful here.\n",
        "\n",
        "    some recommendations say only the positive smoothing (for real = 1) is needed\n",
        "    but both can be applied\n",
        "'''\n",
        "\n",
        "from numpy.random import random\n",
        "from numpy import ones\n",
        "from numpy import zeros\n",
        "\n",
        "def smooth_positive(y_pos):\n",
        "    return y_pos - 0.3 + (random(y_pos.shape) * 0.5)  # gives values in (0.7, 1.2)\n",
        "\n",
        "y_pos = ones((100,1)) # array of 100 samples, each of single values, all 100 are 1\n",
        "\n",
        "y_p = smooth_positive(y_pos)\n",
        "\n",
        "print(y_p.shape, y_p.min(), y_p.max())\n",
        "\n",
        "def smooth_negative(y_neg):\n",
        "    return y_neg + 0.3 * random(y_neg.shape)\n",
        "\n",
        "y_neg = zeros((100,1)) # array of 100 samples, each of single values, all 100 are 0\n",
        "\n",
        "y_n = smooth_negative(y_neg)\n",
        "\n",
        "print(y_n.shape, y_n.min(), y_n.max())  # gives values in (0, 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbjXHXDT--h6",
        "outputId": "f40ca03d-28ce-49d0-999f-c657df8e9425"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1) 0.7103455119233193 1.1998710364500296\n",
            "(100, 1) 0.000386294781397456 0.299211645481458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "4   Using noisy labels for the discriminator.\n",
        "    The labels used when training the discriminator model are always correct. This means that fake\n",
        "    images are always labeled with class 0 and real images are always labeled with class 1. It is\n",
        "    recommended to introduce some errors to these labels where some fake images are marked as\n",
        "    real, and some real images are marked as fake.\n",
        "\n",
        "    the numpy.random.choice() helps with this using its optional 'size' argument\n",
        "\n",
        "    the result below indicates that out of 100 1s, 5 turned to 0s\n",
        "    and vice versa for the 100 0s\n",
        "'''\n",
        "\n",
        "from numpy import ones\n",
        "from numpy import zeros\n",
        "from numpy.random import choice\n",
        "\n",
        "def noisy_labels(y, p_flip):\n",
        "    # number of indices in y to flip the values of\n",
        "    n_flips = int(p_flip * y.shape[0])\n",
        "\n",
        "    # indices to flip values in y\n",
        "    flip_i = choice([i for i in range(y.shape[0])], size = n_flips)\n",
        "\n",
        "    # flip values\n",
        "    y[flip_i] = 1 - y[flip_i]\n",
        "\n",
        "    return y\n",
        "\n",
        "n_samples = 100\n",
        "real = ones((n_samples, 1))\n",
        "\n",
        "noisy_real = noisy_labels(real, 0.05) # flip with 5% probablity\n",
        "\n",
        "print(noisy_real.sum())\n",
        "\n",
        "fake = zeros((n_samples, 1))\n",
        "\n",
        "noisy_fake = noisy_labels(fake, 0.05)\n",
        "\n",
        "print(noisy_fake.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ISi47vPsBO",
        "outputId": "9e137453-e89d-4fd6-c764-7d6c8db22dd3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95.0\n",
            "5.0\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "###      THIS NOTEBOOK IS BEING REVISED        ###"
      ],
      "metadata": {
        "id": "zTKrELCExh6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import vstack"
      ],
      "metadata": {
        "id": "v43mZj27_xIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_D(in_shape = (28, 28, 1)):\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3,3), strides = (2,2), padding = 'same', input_shape = in_shape),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Dropout(0.4),\n",
        "        Conv2D(64, (3,3), strides = (2,2), padding = 'same'),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Dropout(0.4),\n",
        "        Flatten(),\n",
        "        Dense(1, activation = 'sigmoid')\n",
        "    ])\n",
        "\n",
        "    opt = Adam(learning_rate = 0.0002, beta_1 = 0.5)\n",
        "\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = (['accuracy']))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "iE9D7GXMg58r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_G(latent_dims):\n",
        "\n",
        "    n_nodes = 128 * 7 * 7 # 128 images of 7x7\n",
        "    model = Sequential([\n",
        "        Dense(n_nodes, input_dim = latent_dims),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Reshape((7, 7, 128)),\n",
        "        Conv2DTranspose(128, (4, 4), strides = (2,2), padding = 'same'),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Conv2DTranspose(128, (4, 4), strides = (2,2), padding = 'same'),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Conv2D(1, (7,7),  activation = 'sigmoid', padding = 'same')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EUvbGbUaY2o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_GAN(G, D):\n",
        "\n",
        "    D.trainable = False\n",
        "\n",
        "    gan = Sequential()\n",
        "    gan.add(G)\n",
        "    gan.add(D)\n",
        "\n",
        "    opt = Adam(learning_rate = 0.0002, beta_1 = 0.5)\n",
        "\n",
        "    gan.compile(loss = 'binary_crossentropy', optimizer = opt)\n",
        "\n",
        "    return gan"
      ],
      "metadata": {
        "id": "8EOMG0OWzUet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_real_samples():\n",
        "\n",
        "    (Xt, _), (_, _) = load_data()\n",
        "\n",
        "    X = expand_dims(Xt, axis = -1)\n",
        "\n",
        "    X = X.astype('float32')\n",
        "    X = X / 255.0\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "Wop4x-MRmyVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "\n",
        "    i = randint(0, dataset.shape[0], n_samples)\n",
        "    X = dataset[i]\n",
        "    y = ones((n_samples, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "rYFiiz_YLBLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dims, n_samples):\n",
        "\n",
        "    X = randn(latent_dims * n_samples)\n",
        "    X = X.reshape((n_samples, latent_dims))\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "Y2G6RhPNi-_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(G, latent_dims, n_samples):\n",
        "\n",
        "    L = generate_latent_points(latent_dims, n_samples)\n",
        "\n",
        "    X = G.predict(L, verbose = None)\n",
        "\n",
        "    y = zeros((n_samples, 1))\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "kE4aUjJhkUZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(examples, epoch, n = 10):\n",
        "\n",
        "    for i in range(n * n):\n",
        "\n",
        "        plt.subplot(n, n, i + 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(examples[i, :, :, 0], cmap = 'gray')\n",
        "\n",
        "    filename = 'gen_plot_%03d.png' % (epoch+1)\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "p8qrx1w56YXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_performance(dataset, D, G, epoch, latent_dim, n_samples = 100):\n",
        "\n",
        "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\n",
        "    _, acc_real = D.evaluate(X_real, y_real, verbose = 0)\n",
        "\n",
        "    X_fake, y_fake = generate_fake_samples(G, latent_dim, n_samples)\n",
        "\n",
        "    _, acc_fake = D.evaluate(X_fake, y_fake, verbose = 0)\n",
        "\n",
        "    print(f'Acc Real : {acc_real} -- Acc Fake : {acc_fake}')\n",
        "\n",
        "    save_images(X_fake, epoch)\n",
        "\n",
        "    filename = 'gen_mnist_%03d.h5' % (epoch+1)\n",
        "    G.save(filename)"
      ],
      "metadata": {
        "id": "QQzwkQ-O5Kro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_GAN(G, D, GAN, dataset, latent_dim, n_epochs = 100, batch_size = 256):\n",
        "\n",
        "    batch_p_epoch = dataset.shape[0] // batch_size\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch in range(batch_p_epoch):\n",
        "\n",
        "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            X_fake, y_fake = generate_fake_samples(G, latent_dim, half_batch)\n",
        "\n",
        "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
        "\n",
        "            d_loss, _ = D.train_on_batch(X, y)\n",
        "\n",
        "            X_gan = generate_latent_points(latent_dim, batch_size)\n",
        "\n",
        "            y_gan = ones((batch_size, 1))\n",
        "\n",
        "            g_loss = GAN.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            print(f'epoch :{epoch+1} - batch : {batch+1}/{batch_p_epoch} - d_loss : {d_loss} - g_loss : {g_loss}')\n",
        "\n",
        "        if (epoch+1) % 10 == 0 :\n",
        "            summarize_performance(dataset, D, G, epoch, latent_dim)\n"
      ],
      "metadata": {
        "id": "qlYpGkW_21W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dims = 100\n",
        "D = make_D()\n",
        "G = make_G(latent_dims)\n",
        "GAN = make_GAN(G, D)\n",
        "\n",
        "dataset = load_real_samples()\n",
        "\n",
        "train_GAN(G, D, GAN, dataset, latent_dims)"
      ],
      "metadata": {
        "id": "-QjmX0yUB1eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Running the above cells should get you the model weights as h5 for every 10 epochs\n",
        "and the digits generate using each of those.\n",
        "\n",
        "I've put those into the 'DCGAN MNIST Saved files; folder\n",
        "for using them to generate images. This shall be done separately though, not here :-)\n",
        "\n",
        "I will make some changes to try and get better generated digits\n",
        "will update this file with those changes.\n",
        "'''"
      ],
      "metadata": {
        "id": "50iKqZhMJJpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
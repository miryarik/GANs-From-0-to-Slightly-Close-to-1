{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jiaYexh4EC00aZoZcjxD_6bNTEBd2gvg",
      "authorship_tag": "ABX9TyPBXo/4u11k1Jl0meqetaRL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "###      THIS NOTEBOOK IS BEING REVISED        ###"
      ],
      "metadata": {
        "id": "zTKrELCExh6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot as plt\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import vstack"
      ],
      "metadata": {
        "id": "v43mZj27_xIq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_D(in_shape = (28, 28, 1)):\n",
        "\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3,3), strides = (2,2), padding = 'same', input_shape = in_shape),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Dropout(0.4),\n",
        "        Conv2D(64, (3,3), strides = (2,2), padding = 'same'),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Dropout(0.4),\n",
        "        Flatten(),\n",
        "        Dense(1, activation = 'sigmoid')\n",
        "    ])\n",
        "\n",
        "    opt = Adam(learning_rate = 0.0002, beta_1 = 0.5)\n",
        "\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = (['accuracy']))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "iE9D7GXMg58r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_G(latent_dims):\n",
        "\n",
        "    n_nodes = 128 * 7 * 7 # 128 images of 7x7\n",
        "    model = Sequential([\n",
        "        Dense(n_nodes, input_dim = latent_dims),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Reshape((7, 7, 128)),\n",
        "        Conv2DTranspose(128, (4, 4), strides = (2,2), padding = 'same'),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Conv2DTranspose(128, (4, 4), strides = (2,2), padding = 'same'),\n",
        "        LeakyReLU(alpha = 0.2),\n",
        "        Conv2D(1, (7,7),  activation = 'sigmoid', padding = 'same')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EUvbGbUaY2o2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_GAN(G, D):\n",
        "\n",
        "    D.trainable = False\n",
        "\n",
        "    gan = Sequential()\n",
        "    gan.add(G)\n",
        "    gan.add(D)\n",
        "\n",
        "    opt = Adam(learning_rate = 0.0002, beta_1 = 0.5)\n",
        "\n",
        "    gan.compile(loss = 'binary_crossentropy', optimizer = opt)\n",
        "\n",
        "    return gan"
      ],
      "metadata": {
        "id": "8EOMG0OWzUet"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_real_samples():\n",
        "\n",
        "    (Xt, _), (_, _) = load_data()\n",
        "\n",
        "    X = expand_dims(Xt, axis = -1)\n",
        "\n",
        "    X = X.astype('float32')\n",
        "    X = X / 255.0\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "Wop4x-MRmyVb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "\n",
        "    i = randint(0, dataset.shape[0], n_samples)\n",
        "    X = dataset[i]\n",
        "    y = ones((n_samples, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "rYFiiz_YLBLW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dims, n_samples):\n",
        "\n",
        "    X = randn(latent_dims * n_samples)\n",
        "    X = X.reshape((n_samples, latent_dims))\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "Y2G6RhPNi-_K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(G, latent_dims, n_samples):\n",
        "\n",
        "    L = generate_latent_points(latent_dims, n_samples)\n",
        "\n",
        "    X = G.predict(L, verbose = None)\n",
        "\n",
        "    y = zeros((n_samples, 1))\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "kE4aUjJhkUZX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(examples, epoch, n = 10):\n",
        "\n",
        "    for i in range(n * n):\n",
        "\n",
        "        plt.subplot(n, n, i + 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(examples[i, :, :, 0], cmap = 'gray')\n",
        "\n",
        "    filename = 'gen_plot_%03d.png' % (epoch+1)\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "p8qrx1w56YXU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_performance(dataset, D, G, epoch, latent_dim, n_samples = 100):\n",
        "\n",
        "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\n",
        "    _, acc_real = D.evaluate(X_real, y_real, verbose = 0)\n",
        "\n",
        "    X_fake, y_fake = generate_fake_samples(G, latent_dim, n_samples)\n",
        "\n",
        "    _, acc_fake = D.evaluate(X_fake, y_fake, verbose = 0)\n",
        "\n",
        "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\n",
        "    #print(f'Acc Real : {acc_real} -- Acc Fake : {acc_fake}')\n",
        "\n",
        "    save_images(X_fake, epoch)\n",
        "\n",
        "    filename = 'gen_mnist_%03d.h5' % (epoch+1)\n",
        "    G.save(filename)"
      ],
      "metadata": {
        "id": "QQzwkQ-O5Kro"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_GAN(G, D, GAN, dataset, latent_dim, n_epochs = 100, batch_size = 256):\n",
        "\n",
        "    batch_p_epoch = dataset.shape[0] // n_epochs\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch in range(batch_p_epoch):\n",
        "\n",
        "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            X_fake, y_fake = generate_fake_samples(G, latent_dim, half_batch)\n",
        "\n",
        "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
        "\n",
        "            d_loss, _ = D.train_on_batch(X, y)\n",
        "\n",
        "            X_gan = generate_latent_points(latent_dim, batch_size)\n",
        "\n",
        "            y_gan = ones((batch_size, 1))\n",
        "\n",
        "            g_loss = GAN.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (epoch+1, batch+1, batch_p_epoch, d_loss, g_loss))\n",
        "\n",
        "            #print(f'epoch :{epoch+1} - batch : {batch+1}/{batch_p_epoch} - d_loss : {d_loss} - g_loss : {g_loss}')\n",
        "\n",
        "        if (epoch+1) % 10 == 0 :\n",
        "            summarize_performance(dataset, D, G, epoch, latent_dim)\n"
      ],
      "metadata": {
        "id": "qlYpGkW_21W0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dims = 100\n",
        "D = make_D()\n",
        "G = make_G(latent_dims)\n",
        "GAN = make_GAN(G, D)\n",
        "\n",
        "dataset = load_real_samples()\n",
        "\n",
        "train_GAN(G, D, GAN, dataset, latent_dims)"
      ],
      "metadata": {
        "id": "-QjmX0yUB1eZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyML/j8VXIy1CKfT8NhNu4dR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The original DCGAN paper recommends 5 practices for a stable training process"],"metadata":{"id":"P-dJRAXo_c2N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4wagREy_X92"},"outputs":[],"source":["'''\n","   1    Downsample using strided convolutions instead of pooling in the generator.\n","        A stride of (2,2) has the effect of reducing each dimension of the image by half,\n","        effectly resultuing in a feature-man of a quarter size.\n","\n","        for instance, lets say the inputs are colored (3 channel) images, each of size 64 x 64\n","        a conv2d layer of with a (2,2) stride will output an image of 32 x 32 for each of the filters\n","\n","        note that the formula for each output dim is : Output dim_i = ((Input dim_i − Filter dim_ )​ / Strides ) + 1\n","\n","        this means that for a height and width of 64, with the 3,3 kernel size, both height and weight are :\n","\n","        ((64 - 3) / 2 ) + 1 = 61/2 + 1 = 30.5 + 1 = 31.5\n","\n","        the padding = \"same\" parameter will for this to be rounded up (instead of the default of rounding down)\n","        to give an output with dims (32, 32, no. of filters)\n","'''\n","\n","from keras.layers import Conv2D\n","from keras.models import Sequential\n","\n","model = Sequential()\n","\n","model.add(Conv2D(64, (3,3), strides = (2,2), padding = \"same\", input_shape = (64, 64, 3)))\n","model.summary()\n","\n","del(model) # so that i can run this cell multiple times without adding more layers to the same model"]},{"cell_type":"code","source":["'''\n","    1   Upsampling using strided transpose convolutions instead of UpSampling layers in the generator.\n","        A stride of (2,2), which works as output stride for this layer, has the effect of upsampling the\n","        dimensions - in fact doubling them when used with padding = \"same\".\n","\n","        an input of (64,64,3) then produces the output of (128, 128, no. of filters)\n","\n","        recall how upsampling actually works and note that the filters are used to output a smaller matrix for conv2d\n","        while the filters are used to output a smaller matrix for the conv2dtranspose\n","'''"],"metadata":{"id":"UjgJPTjAAooW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.layers import Conv2DTranspose\n","\n","model = Sequential()\n","\n","model.add(Conv2DTranspose(64, (4,4), strides = (2,2), padding = \"same\", input_shape = (64, 63, 3)))\n","\n","model.summary()\n","\n","del(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tXb-qxUFdkS","executionInfo":{"status":"ok","timestamp":1707033037798,"user_tz":-330,"elapsed":14,"user":{"displayName":"Yarik Arshad Mir miryarik","userId":"06832305817001960458"}},"outputId":"25a4ca69-6ab7-49d9-8b6c-d7054c5fe04f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_transpose (Conv2DTr  (None, 128, 126, 64)      3136      \n"," anspose)                                                        \n","                                                                 \n","=================================================================\n","Total params: 3136 (12.25 KB)\n","Trainable params: 3136 (12.25 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]}]}